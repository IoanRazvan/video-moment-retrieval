{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from video_moment_retrieval.utils.logging import init_logging, logger\n",
    "from video_moment_retrieval.testing.modeling_dd import DoubleDecoderModel, DDDataset\n",
    "from video_moment_retrieval.testing.configuration_dd import  DoubleDecoderConfig \n",
    "from transformers import TrainingArguments, Trainer, PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "init_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ioan/.cache/pypoetry/virtualenvs/video-moment-retrieval-rdzoMJ72-py3.10/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1136' max='45200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1136/45200 1:06:41 < 43:11:41, 0.28 it/s, Epoch 5.02/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.535500</td>\n",
       "      <td>2.746895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.624700</td>\n",
       "      <td>2.403987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.355100</td>\n",
       "      <td>2.183485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.163400</td>\n",
       "      <td>2.066316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.051900</td>\n",
       "      <td>2.002259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ioan/.cache/pypoetry/virtualenvs/video-moment-retrieval-rdzoMJ72-py3.10/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/home/ioan/.cache/pypoetry/virtualenvs/video-moment-retrieval-rdzoMJ72-py3.10/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/home/ioan/.cache/pypoetry/virtualenvs/video-moment-retrieval-rdzoMJ72-py3.10/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/home/ioan/.cache/pypoetry/virtualenvs/video-moment-retrieval-rdzoMJ72-py3.10/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/home/ioan/.cache/pypoetry/virtualenvs/video-moment-retrieval-rdzoMJ72-py3.10/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=Tokenizer.from_file(\"tokenizer.json\"),\n",
    "    pad_token=\"[PAD]\",\n",
    "    unk_token=\"[UNK]\"\n",
    ")\n",
    "train_dataset = DDDataset(\"../../qvhighlights_features/highlight_train_release.jsonl\", \"../../qvhighlights_features/bert_features\", \"../../qvhighlights_features/resnet_features\", target_tokenizer=tokenizer)\n",
    "eval_dataset = DDDataset(\"../../qvhighlights_features/highlight_val_release.jsonl\", \"../../qvhighlights_features/bert_features\", \"../../qvhighlights_features/resnet_features\", target_tokenizer=tokenizer)\n",
    "\n",
    "config = DoubleDecoderConfig(\n",
    "    len(tokenizer.vocab),\n",
    "    tokenizer.pad_token_id,\n",
    ")\n",
    "# logger.info(\"Running model using config %s\", config)\n",
    "\n",
    "model = DoubleDecoderModel(config)\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    \"./train_output/double_decoder_2\",\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=\"constant_with_warmup\",\n",
    "    warmup_steps=500,\n",
    "    num_train_epochs=200,\n",
    "    logging_steps=200,\n",
    "    save_steps=200,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,\n",
    "    max_grad_norm=0.1,\n",
    "    label_names=[\"labels\"],\n",
    "    weight_decay=1e-1,\n",
    "    eval_do_concat_batches=False,\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    data_collator=train_dataset.collate,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_model = DoubleDecoderModel.from_pretrained(\"./train_output/double_decoder/checkpoint-10000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=Tokenizer.from_file(\"./tokenizer.json\"),\n",
    "    pad_token=\"[PAD]\",\n",
    "    unk_token=\"[UNK]\"\n",
    ")\n",
    "train_dataset = DDDataset(\"../../qvhighlights_features/highlight_val_release.jsonl\", \"../../qvhighlights_features/bert_features\", \"../../qvhighlights_features/resnet_features\", target_tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(train_dataset, batch_size=1, collate_fn=train_dataset.collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ 14 48 ] ] </s>'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = tokenizer.decode(batch[\"labels\"][0])\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"decoder_input_ids\"] = batch[\"decoder_input_ids\"][:, :1]\n",
    "batch[\"decoder_attention_mask\"] = batch[\"decoder_attention_mask\"][:, :1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "del batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [ 18 70 ] ] </s>\n",
      "[ 82 150 ] ] </s>\n",
      "\n",
      "[ [ 58 104 ] ] </s>\n",
      "[ 118 136 ] ] </s>\n",
      "\n",
      "[ [ 22 40 ] ] </s>\n",
      "[ 56 76 ] [ 96 150 ] ] </s>\n",
      "\n",
      "[ [ 0 114 ] ] </s>\n",
      "[ 36 60 ] ] </s>\n",
      "\n",
      "[ [ 92 118 ] ] </s>\n",
      "[ 78 92 ] ] </s>\n",
      "\n",
      "[ [ 0 150 ] ] </s>\n",
      "[ 0 74 ] [ 76 142 ] [ 144 150 ] ] </s>\n",
      "\n",
      "[ [ 80 150 ] ] </s>\n",
      "[ 44 136 ] ] </s>\n",
      "\n",
      "[ [ 0 16 ] ] </s>\n",
      "[ 10 108 ] ] </s>\n",
      "\n",
      "[ [ 0 22 ] [ 28 64 ] ] </s>\n",
      "[ 0 16 ] ] </s>\n",
      "\n",
      "[ [ 50 60 ] ] </s>\n",
      "[ 0 144 ] ] </s>\n",
      "\n",
      "[ [ 24 46 ] [ 62 78 ] ] </s>\n",
      "[ 6 20 ] ] </s>\n",
      "\n",
      "[ [ 0 18 ] [ 22 32 ] ] </s>\n",
      "[ 0 2 ] [ 52 74 ] [ 84 92 ] ] </s>\n",
      "\n",
      "[ [ 0 92 ] ] </s>\n",
      "[ 52 88 ] [ 96 98 ] ] </s>\n",
      "\n",
      "[ [ 82 44 ] ] </s>\n",
      "[ 34 62 ] ] </s>\n",
      "\n",
      "[ [ 0 128 ] [ 140 150 ] ] </s>\n",
      "[ 26 46 ] ] </s>\n",
      "\n",
      "[ [ 92 124 ] ] </s>\n",
      "[ 106 122 ] ] </s>\n",
      "\n",
      "[ [ 46 122 ] ] </s>\n",
      "[ 40 90 ] ] </s>\n",
      "\n",
      "[ [ 114 124 ] ] </s>\n",
      "[ 0 46 ] ] </s>\n",
      "\n",
      "[ [ 114 118 ] [ 120 122 ] ] </s>\n",
      "[ 0 14 ] ] </s>\n",
      "\n",
      "[ [ 44 84 ] ] </s>\n",
      "[ 32 54 ] ] </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch[\"decoder_input_ids\"] = torch.tensor([[12]], dtype=torch.long)\n",
    "batch[\"decoder_attention_mask\"] = torch.tensor([[1]], dtype=torch.long)\n",
    "if \"labels\" in batch:\n",
    "    del batch[\"labels\"]\n",
    "\n",
    "i = 0\n",
    "for batch in loader:\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break\n",
    "    labels = tokenizer.decode(batch[\"labels\"][0])\n",
    "    del batch[\"labels\"]\n",
    "    batch[\"decoder_input_ids\"] = torch.tensor([[12]], dtype=torch.long)\n",
    "    batch[\"decoder_attention_mask\"] = torch.tensor([[1]], dtype=torch.long)\n",
    "    while batch[\"decoder_input_ids\"][0,-1].item() != tokenizer.vocab[\"</s>\"]:\n",
    "        outputs = dd_model(**batch)\n",
    "        batch[\"decoder_input_ids\"] = torch.cat([batch[\"decoder_input_ids\"], torch.argmax(outputs[0], dim=-1)[..., -1][None, :]] , dim=-1)\n",
    "        batch[\"decoder_attention_mask\"] = torch.cat([batch[\"decoder_attention_mask\"], torch.tensor([[1]], dtype=torch.long) ], dim=-1)\n",
    "        \n",
    "        # print(batch[\"decoder_input_ids\"])\n",
    "        # print(batch[\"decoder_attention_mask\"])\n",
    "    print(tokenizer.decode(batch[\"decoder_input_ids\"][0]))\n",
    "    print(labels)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video-moment-retrieval-rdzoMJ72-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
