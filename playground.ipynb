{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract video features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZzGSP0ySLD0_510.0_660.0\n"
     ]
    }
   ],
   "source": [
    "import ffmpeg\n",
    "import os\n",
    "\n",
    "output_path = \"test-frames\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "output_format = \"test-frames\\\\{video_id}-%03d.jpg\"\n",
    "\n",
    "test_video = \".\\\\qvhilights_videos\\\\videos\\\\ZzGSP0ySLD0_510.0_660.0.mp4\"\n",
    "video_id = os.path.basename(test_video).rsplit(\".\", 1)[0]\n",
    "print(video_id)\n",
    "video_stream = ffmpeg.input(test_video)\n",
    "output = ffmpeg.output(video_stream, output_format.format(video_id=video_id), r=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "534"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffmpeg.probe(test_video)[\"streams\"][0][\"width\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ioanr\\miniconda3\\envs\\master\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPImageProcessor, CLIPVisionModel, CLIPTextModel, CLIPTokenizerFast\n",
    "\n",
    "model_checkpoint = \"openai/clip-vit-base-patch32\"\n",
    "model = CLIPVisionModel.from_pretrained(model_checkpoint).to(\"cuda\")\n",
    "model.eval()\n",
    "text_model = CLIPTextModel.from_pretrained(model_checkpoint)\n",
    "text_model.eval()\n",
    "processor = CLIPImageProcessor.from_pretrained(model_checkpoint)\n",
    "text_processor = CLIPTokenizerFast.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_video(video_path: str, fps=0.5) -> npt.NDArray[np.float32]:\n",
    "    video_info = ffmpeg.probe(video_path)\n",
    "    frame_width = video_info[\"streams\"][0][\"width\"]\n",
    "    frame_height = video_info[\"streams\"][0][\"height\"]\n",
    "    \n",
    "    process = ffmpeg.input(\n",
    "        video_path\n",
    "    ).output(\"pipe:\", r=fps, format='rawvideo', pix_fmt=\"rgb24\"\n",
    "    ).run_async(pipe_stdout=True, pipe_stderr=True)\n",
    "    video_array = []\n",
    "    \n",
    "    while True:\n",
    "        in_bytes = process.stdout.read(frame_width * frame_height * 3)\n",
    "        if not in_bytes:\n",
    "            break\n",
    "        in_frame = np.frombuffer(in_bytes, np.uint8).reshape((frame_height, frame_width, 3))\n",
    "        video_array.append(in_frame)\n",
    "    video_array = np.array(video_array, dtype=np.uint8)\n",
    "    # process.kill()\n",
    "    model_input = processor(video_array, return_tensors=\"pt\").to(\"cuda\")\n",
    "    model_output = model(**model_input)\n",
    "    return model_output.pooler_output.cpu().numpy()\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_text(text: str) -> npt.NDArray[np.float32]:\n",
    "    model_input = text_processor(text, return_tensors=\"pt\")\n",
    "    model_output = text_model(**model_input)\n",
    "    return model_output.last_hidden_state.squeeze(0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'subprocess.Popen'>\n",
      "(75, 768)\n"
     ]
    }
   ],
   "source": [
    "video_path = \".\\\\qvhilights_videos\\\\videos\\\\ZzGSP0ySLD0_510.0_660.0.mp4\"\n",
    "encoded_video = encode_video(video_path, 0.49)\n",
    "print(encoded_video.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 512)\n"
     ]
    }
   ],
   "source": [
    "encoded_text = encode_text('A video covering hill and water from a boat')\n",
    "print(encoded_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'qid': 3158, 'query': 'A video covering hill and water from a boat', 'duration': 150, 'vid': '_6hnl_BrFvs_360.0_510.0'}\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "\n",
    "with jsonlines.open(\".\\\\qvhilights_videos\\\\highlight_test_release.jsonl\") as reader:\n",
    "    for el in reader:\n",
    "        print(el)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import tqdm\n",
    "\n",
    "def encode_dataset(dataset_path: str, dataset_info_file: str, output_path: str) -> None:\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    video_features_output_path = os.path.join(output_path, \"video_features\")\n",
    "    os.makedirs(video_features_output_path, exist_ok=True)\n",
    "    \n",
    "    text_features_output_path = os.path.join(output_path, \"text_features\")\n",
    "    os.makedirs(text_features_output_path, exist_ok=True)\n",
    "    \n",
    "    with jsonlines.open(dataset_info_file) as reader:\n",
    "        for el in tqdm.tqdm(reader):\n",
    "            video_in_path = os.path.join(dataset_path, el[\"vid\"] + \".mp4\")\n",
    "            video_out_path = os.path.join(video_features_output_path, el[\"vid\"])\n",
    "            text_out_path = os.path.join(text_features_output_path, str(el[\"qid\"]))\n",
    "            \n",
    "            if not os.path.exists(video_out_path):\n",
    "                video_features = encode_video(video_in_path, fps=0.49)\n",
    "                np.save(video_out_path, video_features)\n",
    "            \n",
    "            if not os.path.exists(text_out_path):\n",
    "                text_features = encode_text(el[\"query\"])\n",
    "                np.save(text_out_path, text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_dataset(\".\\\\qvhilights_videos\\\\videos\", \".\\\\qvhilights_videos\\\\highlight_val_release.jsonl\", \"dis_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MomentDETR Own Implementation\n",
    "\n",
    "### TODO\n",
    "\n",
    "- [ ] Add saliency loss\n",
    "- [ ] Add evaluation metrics\n",
    "- [ ] Match paper performance (implies comparing with it and fixing or adding anything missing)\n",
    "- [ ] Refactor code (cloesly follow HuggingFace model?) (also keep in mind what can be reused between different SOTAs)\n",
    "- [ ] WandB/Tensorboard interface? (easy iteration and rersults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import os\n",
    "import jsonlines\n",
    "from typing import Any\n",
    "\n",
    "QVDataPointTarget = dict[str, Any]\n",
    "QVDataPoint = tuple[npt.NDArray[np.float32], npt.NDArray[np.float32], None | QVDataPointTarget]\n",
    "\n",
    "class QVDataset(Dataset):\n",
    "    def __init__(self, text_features_path: str, video_features_path: str, data_file_path: str):\n",
    "        super().__init__()\n",
    "        self.text_features_path = text_features_path\n",
    "        self.video_features_path = video_features_path\n",
    "        self.data = []\n",
    "        with jsonlines.open(data_file_path) as reader:\n",
    "            for el in reader:\n",
    "                self.data.append(el)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> QVDataPoint:\n",
    "        item_info = self.data[index]\n",
    "        qid = item_info[\"qid\"]\n",
    "        vid = item_info[\"vid\"]\n",
    "        query_features = np.load(os.path.join(self.text_features_path, str(qid) + \".npy\"))\n",
    "        video_features = np.load(os.path.join(self.video_features_path, vid + \".npy\"))\n",
    "        label = None\n",
    "        if \"relevant_clip_ids\" in item_info:\n",
    "            label = {\n",
    "                \"relevant_clip_ids\": item_info[\"relevant_clip_ids\"], \n",
    "                \"saliency_scores\": item_info[\"saliency_scores\"], \n",
    "                \"relevant_windows\": item_info[\"relevant_windows\"],\n",
    "                \"duration\": item_info[\"duration\"]\n",
    "            }\n",
    "        return (query_features, video_features, item_info[\"query\"], label)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# define Data collator function\n",
    "def pad_collate(samples: list[QVDataPoint]):\n",
    "    # Need to: pad video and text features -> create attention video/text attention\n",
    "    text_lens = [el[0].shape[0] for el in samples]\n",
    "    video_lens = [el[1].shape[0] for el in samples]\n",
    "    \n",
    "    batch_size = len(samples)\n",
    "    text_len = max(text_lens)\n",
    "    text_hidden = samples[0][0].shape[-1]\n",
    "    video_len = max(video_lens)\n",
    "    video_hidden = samples[0][1].shape[-1]\n",
    "    \n",
    "    text_features = torch.zeros((batch_size, text_len, text_hidden))\n",
    "    text_attn_mask = torch.ones((batch_size, text_len))\n",
    "    video_features = torch.zeros((batch_size, video_len, video_hidden))\n",
    "    video_attn_mask = torch.ones((batch_size, video_len))\n",
    "    \n",
    "    for idx, sample in enumerate(samples):\n",
    "        sample_text_len = sample[0].shape[0]\n",
    "        sample_video_len = sample[1].shape[0]\n",
    "        text_features[idx, :sample_text_len, :] = torch.tensor(sample[0])\n",
    "        video_features[idx, :sample_video_len, :] = torch.tensor(sample[1])\n",
    "        text_attn_mask[idx, sample_text_len:] = 0\n",
    "        video_attn_mask[idx, sample_video_len:] = 0\n",
    "    \n",
    "    labels = None\n",
    "    # We have labels\n",
    "    if samples[0][2] is not None:\n",
    "        # build classes array\n",
    "        labels = []\n",
    "        for (_, _, sample) in samples:\n",
    "            duration = sample[\"duration\"]\n",
    "            boxes = sample[\"relevant_windows\"]\n",
    "            class_labels = torch.zeros((len(boxes), ), dtype=torch.int64) \n",
    "            labels.append({\n",
    "                \"boxes\": torch.tensor([[box[0] / duration, box[1] / duration] for box in boxes]),\n",
    "                \"class_labels\": class_labels\n",
    "            })\n",
    "    \n",
    "    return  {\n",
    "        \"text_features\": text_features,\n",
    "        \"text_attn_mask\": text_attn_mask,\n",
    "        \"video_features\": video_features,\n",
    "        \"video_attn_mask\": video_attn_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.33928594,  0.11646017,  0.10195109, ...,  0.24677397,\n",
       "          0.5906364 ,  0.10129976],\n",
       "        [ 0.5391787 ,  0.6050957 , -0.29105273, ...,  0.11356771,\n",
       "          0.07524773, -1.3233696 ],\n",
       "        [ 0.34825635, -0.5755961 , -0.18056145, ...,  1.5290366 ,\n",
       "          0.17571366, -0.41787058],\n",
       "        ...,\n",
       "        [ 0.78474975,  0.6386728 , -0.09147779, ..., -0.27714124,\n",
       "         -1.1072998 ,  0.3630575 ],\n",
       "        [ 0.50689864, -0.25536713, -0.9592153 , ...,  0.5298033 ,\n",
       "          1.6603259 , -0.7617836 ],\n",
       "        [ 0.72716683, -0.6981834 ,  0.03235934, ...,  1.5555809 ,\n",
       "          0.70241404, -0.5428549 ]], dtype=float32),\n",
       " array([[ 0.00542442,  1.2323831 ,  0.596713  , ..., -0.30323648,\n",
       "          1.965467  , -0.3933972 ],\n",
       "        [ 0.00542442,  1.2323831 ,  0.596713  , ..., -0.30323648,\n",
       "          1.965467  , -0.3933972 ],\n",
       "        [-0.09869489,  0.5870662 ,  0.19045241, ..., -1.0922167 ,\n",
       "          1.7422361 , -0.04100407],\n",
       "        ...,\n",
       "        [-0.47089767, -0.5834327 , -1.2728491 , ...,  0.14045978,\n",
       "          2.1142485 ,  0.55699825],\n",
       "        [-0.48554724, -0.48496026, -0.33971408, ..., -0.07884081,\n",
       "          0.9604795 ,  0.6867931 ],\n",
       "        [-0.0324238 ,  0.01446079, -0.63984007, ..., -0.4554176 ,\n",
       "          1.3922255 ,  0.86437327]], dtype=float32),\n",
       " {'relevant_clip_ids': [36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   54,\n",
       "   55,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   72],\n",
       "  'saliency_scores': [[4, 3, 2],\n",
       "   [4, 1, 3],\n",
       "   [4, 3, 4],\n",
       "   [4, 1, 2],\n",
       "   [4, 2, 2],\n",
       "   [4, 2, 2],\n",
       "   [4, 2, 2],\n",
       "   [4, 2, 2],\n",
       "   [4, 1, 2],\n",
       "   [4, 1, 3],\n",
       "   [4, 3, 4],\n",
       "   [4, 3, 3],\n",
       "   [4, 3, 3],\n",
       "   [4, 3, 4],\n",
       "   [4, 3, 2],\n",
       "   [4, 1, 2],\n",
       "   [4, 3, 2],\n",
       "   [4, 1, 2],\n",
       "   [4, 1, 2],\n",
       "   [4, 1, 2],\n",
       "   [4, 1, 2],\n",
       "   [3, 3, 3],\n",
       "   [4, 3, 4],\n",
       "   [4, 1, 3],\n",
       "   [4, 1, 2],\n",
       "   [4, 1, 2],\n",
       "   [4, 1, 2],\n",
       "   [4, 1, 2],\n",
       "   [4, 1, 2]],\n",
       "  'relevant_windows': [[72, 82],\n",
       "   [84, 94],\n",
       "   [96, 106],\n",
       "   [108, 118],\n",
       "   [120, 130],\n",
       "   [136, 142],\n",
       "   [144, 146]],\n",
       "  'duration': 150})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = QVDataset(\"qvhighlights_features\\\\text_features\", \"qvhighlights_features\\\\video_features\", \"qvhighlights_features\\\\highlight_train_release.jsonl\")\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_features': tensor([[[ 0.3393,  0.1165,  0.1020,  ...,  0.2468,  0.5906,  0.1013],\n",
       "          [ 1.9753, -0.5844,  0.3685,  ...,  1.1658,  0.8050, -0.9801],\n",
       "          [ 1.0568, -0.3584, -0.1190,  ...,  0.4568, -0.9863,  0.2836],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.3393,  0.1165,  0.1020,  ...,  0.2468,  0.5906,  0.1013],\n",
       "          [ 1.9753, -0.5844,  0.3685,  ...,  1.1658,  0.8050, -0.9801],\n",
       "          [ 0.0405, -0.4019, -0.2012,  ...,  0.5851,  0.2978, -2.3808],\n",
       "          ...,\n",
       "          [-0.1568, -0.2916, -0.0831,  ..., -0.3611,  0.4270, -0.1416],\n",
       "          [-0.0660, -0.6583, -0.6306,  ...,  0.3031, -1.0054,  0.6075],\n",
       "          [ 1.0487, -0.2953, -0.0168,  ...,  0.9910, -0.9835, -0.6988]]]),\n",
       " 'text_attn_mask': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]),\n",
       " 'video_features': tensor([[[-1.7489,  1.4548, -0.9394,  ...,  1.1484,  1.7676, -0.7794],\n",
       "          [-1.0687,  0.9082, -1.1643,  ...,  0.1063,  1.5667,  0.5823],\n",
       "          [-0.7873,  1.8033, -1.0241,  ...,  0.4660,  1.4297, -0.3257],\n",
       "          ...,\n",
       "          [-0.2117,  0.0181, -0.7275,  ...,  0.3303,  0.9148, -0.1047],\n",
       "          [-0.2406,  0.4505, -0.8144,  ..., -0.0493,  0.9036, -0.3508],\n",
       "          [-0.3656,  1.5215, -0.9557,  ..., -0.2955,  0.8541,  0.2608]],\n",
       " \n",
       "         [[-0.0652,  1.1574, -0.5155,  ..., -0.0367,  1.9767, -0.2036],\n",
       "          [-0.0600,  1.1549, -0.5124,  ..., -0.0309,  1.9815, -0.1897],\n",
       "          [-1.2995,  1.5676,  0.3723,  ..., -1.1579,  0.8864, -0.0791],\n",
       "          ...,\n",
       "          [-0.4934, -0.5436, -1.6220,  ...,  0.0616,  0.7477, -0.9031],\n",
       "          [-0.1501,  0.2135, -1.3042,  ...,  0.2034,  1.3097, -0.0849],\n",
       "          [-0.3000,  1.3071, -1.1724,  ...,  0.2121,  0.6282, -1.1873]]]),\n",
       " 'video_attn_mask': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1.]]),\n",
       " 'labels': [{'boxes': tensor([[0.1467, 0.6533]]), 'class_labels': tensor([0])},\n",
       "  {'boxes': tensor([[0.6000, 0.6400],\n",
       "           [0.6667, 0.7333]]),\n",
       "   'class_labels': tensor([0, 0])}]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(12)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_attn_mask = batch[\"text_attn_mask\"]\n",
    "print(text_attn_mask)\n",
    "torch.argmax((text_attn_mask < 0.5).to(torch.int8), dim=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 75, 768])\n"
     ]
    }
   ],
   "source": [
    "print(batch[\"video_features\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DetrConfig, DetrForObjectDetection, DetrModel\n",
    "from transformers.models.detr.modeling_detr import DetrEncoder, DetrDecoder, DetrSinePositionEmbedding\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from typing import Optional\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "\n",
    "\n",
    "# taken from https://github.com/facebookresearch/detr/blob/master/models/matcher.py\n",
    "class MomentDetrHungarianMatcher(nn.Module):\n",
    "    \"\"\"\n",
    "    This class computes an assignment between the targets and the predictions of the network.\n",
    "\n",
    "    For efficiency reasons, the targets don't include the no_object. Because of this, in general, there are more\n",
    "    predictions than targets. In this case, we do a 1-to-1 matching of the best predictions, while the others are\n",
    "    un-matched (and thus treated as non-objects).\n",
    "\n",
    "    Args:\n",
    "        class_cost:\n",
    "            The relative weight of the classification error in the matching cost.\n",
    "        bbox_cost:\n",
    "            The relative weight of the L1 error of the bounding box coordinates in the matching cost.\n",
    "        giou_cost:\n",
    "            The relative weight of the giou loss of the bounding box in the matching cost.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, class_cost: float = 1, bbox_cost: float = 1, giou_cost: float = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.class_cost = class_cost\n",
    "        self.bbox_cost = bbox_cost\n",
    "        self.giou_cost = giou_cost\n",
    "        if class_cost == 0 and bbox_cost == 0 and giou_cost == 0:\n",
    "            raise ValueError(\"All costs of the Matcher can't be 0\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs: torch.FloatTensor, targets: list[dict]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            outputs (`dict`):\n",
    "                A dictionary that contains at least these entries:\n",
    "                * \"logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n",
    "                * \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates.\n",
    "            targets (`List[dict]`):\n",
    "                A list of targets (len(targets) = batch_size), where each target is a dict containing:\n",
    "                * \"class_labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of\n",
    "                  ground-truth\n",
    "                 objects in the target) containing the class labels\n",
    "                * \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates.\n",
    "\n",
    "        Returns:\n",
    "            `List[Tuple]`: A list of size `batch_size`, containing tuples of (index_i, index_j) where:\n",
    "            - index_i is the indices of the selected predictions (in order)\n",
    "            - index_j is the indices of the corresponding selected targets (in order)\n",
    "            For each batch element, it holds: len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n",
    "        \"\"\"\n",
    "        batch_size, num_queries = outputs[\"logits\"].shape[:2]\n",
    "\n",
    "        # We flatten to compute the cost matrices in a batch\n",
    "        out_prob = outputs[\"logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
    "        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 2]\n",
    "\n",
    "        # Also concat the target labels and boxes\n",
    "        target_ids = torch.cat([v[\"class_labels\"] for v in targets])\n",
    "        target_bbox = torch.cat([v[\"boxes\"] for v in targets])\n",
    "\n",
    "        # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n",
    "        # but approximate it in 1 - proba[target class].\n",
    "        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
    "        class_cost = -out_prob[:, target_ids]\n",
    "\n",
    "        # Compute the L1 cost between boxes\n",
    "        bbox_cost = torch.cdist(out_bbox, target_bbox, p=1)\n",
    "\n",
    "        # Compute the giou cost between boxes\n",
    "        giou_cost = -generalized_moment_iou(center_to_corners_format(out_bbox), center_to_corners_format(target_bbox))\n",
    "\n",
    "        # Final cost matrix\n",
    "        cost_matrix = self.bbox_cost * bbox_cost + self.class_cost * class_cost + self.giou_cost * giou_cost\n",
    "        cost_matrix = cost_matrix.view(batch_size, num_queries, -1).cpu()\n",
    "\n",
    "        sizes = [len(v[\"boxes\"]) for v in targets]\n",
    "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(cost_matrix.split(sizes, -1))]\n",
    "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n",
    "\n",
    "def center_to_corners_format(box: torch.FloatTensor) -> torch.FloatTensor:\n",
    "    center, width = box.unbind(-1)\n",
    "    return torch.stack(\n",
    "        [center - 0.5 * width, center + 0.5 * width],\n",
    "        dim=-1\n",
    "    )\n",
    "\n",
    "def generalized_moment_iou(box1: torch.FloatTensor, box2: torch.FloatTensor) -> torch.FloatTensor:\n",
    "    def moment_iou(box1: torch.FloatTensor, box2: torch.FloatTensor) -> tuple[torch.FloatTensor, torch.FloatTensor]:\n",
    "        # box1 [batch * queries, 2]\n",
    "        # box2 [num_target, 2]\n",
    "        length1 = (box1[:, 1] - box1[:, 0])\n",
    "        length2 = (box2[:, 1] - box2[:, 0])\n",
    "        left = torch.max(box1[:, None, 0], box2[:, 0])\n",
    "        right = torch.min(box1[:, None, 1], box2[:, 1])\n",
    "        intersection = (right - left).clamp(min = 0)\n",
    "        union = length1[:, None] + length2 - intersection\n",
    "        \n",
    "        return intersection / union, union \n",
    "    \n",
    "    iou, union = moment_iou(box1, box2)\n",
    "    left = torch.min(box1[:, None, 0], box2[:, 0])\n",
    "    right = torch.max(box1[:, None, 1], box2[:, 1])\n",
    "    area = right - left\n",
    "    return iou - (area - union) / area\n",
    "        \n",
    "class MomentDetrLoss(nn.Module):\n",
    "    def __init__(self, matcher, num_classes, eos_coef, losses):\n",
    "        super().__init__()\n",
    "        self.matcher = matcher\n",
    "        self.num_classes = num_classes\n",
    "        self.eos_coef = eos_coef\n",
    "        self.losses = losses\n",
    "        empty_weight = torch.ones(self.num_classes + 1)\n",
    "        empty_weight[-1] = self.eos_coef\n",
    "        self.register_buffer(\"empty_weight\", empty_weight)\n",
    "    \n",
    "    def loss_labels(self, outputs, targets, indices, num_boxes):\n",
    "        \"\"\"\n",
    "        Classification loss (NLL) targets dicts must contain the key \"class_labels\" containing a tensor of dim\n",
    "        [nb_target_boxes]\n",
    "        \"\"\"\n",
    "        if \"logits\" not in outputs:\n",
    "            raise KeyError(\"No logits were found in the outputs\")\n",
    "        source_logits = outputs[\"logits\"]\n",
    "\n",
    "        idx = self._get_source_permutation_idx(indices)\n",
    "        target_classes_o = torch.cat([t[\"class_labels\"][J] for t, (_, J) in zip(targets, indices)])\n",
    "        target_classes = torch.full(\n",
    "            source_logits.shape[:2], self.num_classes, dtype=torch.int64, device=source_logits.device\n",
    "        )\n",
    "        target_classes[idx] = target_classes_o\n",
    "\n",
    "        loss_ce = nn.functional.cross_entropy(source_logits.transpose(1, 2), target_classes, self.empty_weight)\n",
    "        losses = {\"loss_ce\": loss_ce}\n",
    "\n",
    "        return losses\n",
    "\n",
    "    \n",
    "    def loss_boxes(self, outputs, targets, indices, num_boxes):\n",
    "        \"\"\"\n",
    "        Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss.\n",
    "\n",
    "        Targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]. The target boxes\n",
    "        are expected in format (center_x, center_y, w, h), normalized by the image size.\n",
    "        \"\"\"\n",
    "        if \"pred_boxes\" not in outputs:\n",
    "            raise KeyError(\"No predicted boxes found in outputs\")\n",
    "        idx = self._get_source_permutation_idx(indices)\n",
    "        source_boxes = outputs[\"pred_boxes\"][idx]\n",
    "        target_boxes = torch.cat([t[\"boxes\"][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
    "\n",
    "        loss_bbox = nn.functional.l1_loss(source_boxes, target_boxes, reduction=\"none\")\n",
    "\n",
    "        losses = {}\n",
    "        losses[\"loss_bbox\"] = loss_bbox.sum() / num_boxes\n",
    "\n",
    "        loss_giou = 1 - torch.diag(\n",
    "            generalized_moment_iou(center_to_corners_format(source_boxes), center_to_corners_format(target_boxes))\n",
    "        )\n",
    "        losses[\"loss_giou\"] = loss_giou.sum() / num_boxes\n",
    "        return losses\n",
    "\n",
    "    def _get_source_permutation_idx(self, indices):\n",
    "        # permute predictions following indices\n",
    "        batch_idx = torch.cat([torch.full_like(source, i) for i, (source, _) in enumerate(indices)])\n",
    "        source_idx = torch.cat([source for (source, _) in indices])\n",
    "        return batch_idx, source_idx\n",
    "\n",
    "    def get_loss(self, loss, outputs, targets, indices, num_boxes):\n",
    "        loss_map = {\n",
    "            \"labels\": self.loss_labels,\n",
    "            # \"cardinality\": self.loss_cardinality,\n",
    "            \"boxes\": self.loss_boxes,\n",
    "            # \"masks\": self.loss_masks,\n",
    "        }\n",
    "        if loss not in loss_map:\n",
    "            raise ValueError(f\"Loss {loss} not supported\")\n",
    "        return loss_map[loss](outputs, targets, indices, num_boxes)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        This performs the loss computation.\n",
    "\n",
    "        Args:\n",
    "             outputs (`dict`, *optional*):\n",
    "                Dictionary of tensors, see the output specification of the model for the format.\n",
    "             targets (`List[dict]`, *optional*):\n",
    "                List of dicts, such that `len(targets) == batch_size`. The expected keys in each dict depends on the\n",
    "                losses applied, see each loss' doc.\n",
    "        \"\"\"\n",
    "        outputs_without_aux = {k: v for k, v in outputs.items() if k != \"auxiliary_outputs\"}\n",
    "\n",
    "        # Retrieve the matching between the outputs of the last layer and the targets\n",
    "        indices = self.matcher(outputs_without_aux, targets)\n",
    "\n",
    "        # Compute the average number of target boxes across all nodes, for normalization purposes\n",
    "        num_boxes = sum(len(t[\"class_labels\"]) for t in targets)\n",
    "        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
    "        world_size = 1\n",
    "        num_boxes = torch.clamp(num_boxes / world_size, min=1).item()\n",
    "\n",
    "        # Compute all the requested losses\n",
    "        losses = {}\n",
    "        for loss in self.losses:\n",
    "            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n",
    "        \n",
    "        return losses\n",
    "\n",
    "def positional_encodings(input_embs: torch.FloatTensor, n = 10_000):\n",
    "    batch_size, seq_len, d = input_embs.shape\n",
    "    position = torch.arange(0, seq_len).unsqueeze_(1)\n",
    "    denominator = torch.pow(n, 2 * torch.arange(0, d//2) / d)\n",
    "    \n",
    "    pos_enc = position / denominator\n",
    "    encodings = torch.zeros((seq_len, d))\n",
    "    encodings[:, 0::2] = pos_enc.sin()\n",
    "    encodings[:, 1::2] = pos_enc.cos()\n",
    "    return encodings.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "class ProjectionMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, n_layers: int = 2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        h = [hidden_dim] * (n_layers - 1)\n",
    "        for in_dim, out_dim in zip([input_dim] + h, h + [output_dim]):\n",
    "            self.layers.append(nn.LayerNorm(normalized_shape=in_dim))\n",
    "            self.layers.append(nn.Dropout())\n",
    "            self.layers.append(nn.Linear(in_dim, out_dim))\n",
    "            self.layers.append(nn.ReLU())\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class MomentDetr(PreTrainedModel):\n",
    "    def __init__(self, config: DetrConfig):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        self.encoder = DetrEncoder(config)\n",
    "        self.decoder = DetrDecoder(config)\n",
    "        self.positions = DetrSinePositionEmbedding(normalize=True)\n",
    "        \n",
    "        self.text_projection = ProjectionMLP(512, config.hidden_size, config.hidden_size)\n",
    "        self.video_projection = ProjectionMLP(768, config.hidden_size, config.hidden_size)\n",
    "        \n",
    "        self.object_queries = nn.Embedding(config.num_queries, config.d_model)\n",
    "        \n",
    "        # Define saliency score predictor head\n",
    "        self.sal_predictor = nn.Linear(config.hidden_size, 1)\n",
    "        \n",
    "        # Define window bounds predictor head\n",
    "        self.moment_predictor = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.hidden_size, 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Define window class predictor head\n",
    "        self.classifier = nn.Linear(config.hidden_size, 2)\n",
    "        self.post_init()\n",
    "        \n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        video_features: torch.FloatTensor,\n",
    "        video_attn_mask: Optional[torch.FloatTensor],\n",
    "        text_features: torch.FloatTensor,\n",
    "        text_attn_mask: Optional[torch.FloatTensor],\n",
    "        labels: Optional[list[dict]] = None\n",
    "    ) -> torch.FloatTensor:\n",
    "        batch_size, video_seq_len, _ = video_features.shape\n",
    "        _, text_seq_len, _ = text_features.shape\n",
    "        \n",
    "        if video_attn_mask is None:\n",
    "            video_attn_mask = torch.ones((batch_size, video_seq_len), device = video_features.device)\n",
    "        if text_attn_mask is None:\n",
    "            text_attn_mask = torch.ones((batch_size, text_seq_len), device = text_features.device)\n",
    "        \n",
    "        text_projected = self.text_projection(text_features)\n",
    "        video_projected = self.video_projection(video_features)\n",
    "        \n",
    "        concatenated_features = torch.cat([video_projected, text_projected], dim=1)\n",
    "        attn_mask = torch.cat([video_attn_mask, text_attn_mask], dim=1)\n",
    "            \n",
    "        # Compute positional encodings\n",
    "        positions = positional_encodings(concatenated_features).to(self.device)\n",
    "        \n",
    "        # Pass through the encoder using positions and concatenated_features\n",
    "        encoder_output = self.encoder(\n",
    "            inputs_embeds=concatenated_features,\n",
    "            attention_mask=attn_mask,\n",
    "            object_queries=positions,\n",
    "        )\n",
    "        \n",
    "        # Pass through the decoder using positions, object_queries, encoder_output\n",
    "        object_queries = self.object_queries.weight.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        decoder_inputs = torch.zeros_like(object_queries)\n",
    "        \n",
    "        decoder_output = self.decoder(\n",
    "            inputs_embeds=decoder_inputs,\n",
    "            attention_mask=None,\n",
    "            encoder_hidden_states=encoder_output.last_hidden_state,\n",
    "            encoder_attention_mask=attn_mask,\n",
    "            object_queries=positions,\n",
    "            query_position_embeddings=object_queries\n",
    "        )\n",
    "        \n",
    "        pred_moments = self.moment_predictor(decoder_output.last_hidden_state)\n",
    "        logits = self.classifier(decoder_output.last_hidden_state)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            outputs_loss = {\n",
    "                \"logits\": logits,\n",
    "                \"pred_boxes\": pred_moments\n",
    "            }\n",
    "            matcher = MomentDetrHungarianMatcher(self.config.class_cost, self.config.bbox_cost, self.config.giou_cost)\n",
    "            # match_ = matcher(outputs, labels)\n",
    "            criterion = MomentDetrLoss(matcher, self.config.num_labels, self.config.eos_coefficient, [\"labels\", \"boxes\"])\n",
    "            criterion.to(self.device)\n",
    "            \n",
    "            loss_dict = criterion(outputs_loss, labels)\n",
    "            weight_dict = {\n",
    "                \"loss_ce\": 4,  # TODO: Pass this in config\n",
    "                \"loss_bbox\": self.config.bbox_loss_coefficient,\n",
    "                \"loss_giou\": self.config.giou_loss_coefficient\n",
    "            }\n",
    "            loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "\n",
    "        return pred_moments, logits, loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_batch_on_device(batch, device):\n",
    "    on_device_batch = {}\n",
    "    for k, v in batch.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            on_device_batch.update({k: v.to(device)})\n",
    "        else:\n",
    "            on_device_l = []\n",
    "            for el in v:\n",
    "                on_device_dict = {}\n",
    "                for el_k, el_v in el.items():\n",
    "                    on_device_dict.update({el_k: el_v.to(device)})\n",
    "                on_device_l.append(on_device_dict)\n",
    "            on_device_batch.update({k: on_device_l})\n",
    "    return on_device_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000e+00, 1.0283e-01],\n",
       "          [0.0000e+00, 1.2472e-01],\n",
       "          [0.0000e+00, 1.0637e-01],\n",
       "          ...,\n",
       "          [0.0000e+00, 1.4899e-01],\n",
       "          [0.0000e+00, 1.3989e-01],\n",
       "          [0.0000e+00, 1.0796e-01]],\n",
       " \n",
       "         [[0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "         [[0.0000e+00, 1.2454e-01],\n",
       "          [0.0000e+00, 9.5373e-02],\n",
       "          [0.0000e+00, 8.2522e-02],\n",
       "          ...,\n",
       "          [0.0000e+00, 6.0441e-02],\n",
       "          [0.0000e+00, 1.0840e-01],\n",
       "          [0.0000e+00, 1.0471e-01]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 4.0778e-03],\n",
       "          [0.0000e+00, 2.6076e-02],\n",
       "          ...,\n",
       "          [0.0000e+00, 5.1384e-02],\n",
       "          [0.0000e+00, 1.0886e-02],\n",
       "          [0.0000e+00, 7.7266e-02]],\n",
       " \n",
       "         [[0.0000e+00, 1.5711e-02],\n",
       "          [0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 2.7041e-02],\n",
       "          ...,\n",
       "          [0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 1.0358e-02],\n",
       "          [0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "         [[0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 1.6248e-05],\n",
       "          [0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 3.8664e-02]]], device='cuda:0', grad_fn=<ReluBackward0>),\n",
       " tensor([[[-0.3928, -0.0165],\n",
       "          [-0.6108, -0.0025],\n",
       "          [-0.7146, -0.2728],\n",
       "          ...,\n",
       "          [-0.5759, -0.3068],\n",
       "          [-0.4652,  0.2128],\n",
       "          [-0.6225, -0.1242]],\n",
       " \n",
       "         [[-0.3733,  0.5743],\n",
       "          [-0.1819,  0.8559],\n",
       "          [-0.2825,  0.9419],\n",
       "          ...,\n",
       "          [-0.1671,  0.5985],\n",
       "          [-0.8083,  1.1709],\n",
       "          [-0.5558,  0.9590]],\n",
       " \n",
       "         [[-0.3917, -0.1800],\n",
       "          [-0.7319,  0.0278],\n",
       "          [-0.2564, -0.1842],\n",
       "          ...,\n",
       "          [-0.7462, -0.3018],\n",
       "          [-0.5121,  0.0293],\n",
       "          [-0.6245, -0.3815]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.2076,  0.7600],\n",
       "          [-0.2344,  0.4640],\n",
       "          [-0.3884,  0.6953],\n",
       "          ...,\n",
       "          [-0.2497,  0.4338],\n",
       "          [-0.4161,  0.8390],\n",
       "          [ 0.0344,  0.3698]],\n",
       " \n",
       "         [[ 0.2224, -0.4071],\n",
       "          [-0.2036, -0.2768],\n",
       "          [-0.0793, -0.3070],\n",
       "          ...,\n",
       "          [ 0.1135, -0.2525],\n",
       "          [-0.2137, -0.1476],\n",
       "          [-0.0561, -0.0333]],\n",
       " \n",
       "         [[ 0.4115, -0.2557],\n",
       "          [ 0.5514, -0.5343],\n",
       "          [ 0.6064, -0.4861],\n",
       "          ...,\n",
       "          [ 0.1951, -0.2982],\n",
       "          [ 0.2320, -0.5398],\n",
       "          [ 0.3144, -0.3726]]], device='cuda:0', grad_fn=<ViewBackward0>),\n",
       " tensor(13.6086, device='cuda:0', grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = DetrConfig(\n",
    "    d_model=256,\n",
    "    encoder_layers=2,\n",
    "    decoder_layers=2,\n",
    "    num_queries=10,\n",
    "    # TODO: figure out dropout details\n",
    "    dropout=0.1,\n",
    "    activation_dropout=0.1,\n",
    "    giou_loss_coefficient=1,\n",
    "    bbox_loss_coefficient=10,\n",
    "    num_labels=1,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "momentDETR_model = MomentDetr(config).to(\"cuda\")\n",
    "batch = put_batch_on_device(next(iter(train_loader)), \"cuda\")\n",
    "\n",
    "momentDETR_model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 - loss 0.50\n",
      "Epoch #0 - loss 0.74\n",
      "Epoch #0 - avg loss 0.61\n",
      "Epoch #1 - loss 0.77\n",
      "Epoch #1 - loss 0.55\n",
      "Epoch #1 - avg loss 0.63\n",
      "Epoch #2 - loss 0.52\n",
      "Epoch #2 - loss 0.61\n",
      "Epoch #2 - avg loss 0.59\n",
      "Epoch #3 - loss 0.53\n",
      "Epoch #3 - loss 0.55\n",
      "Epoch #3 - avg loss 0.59\n",
      "Epoch #4 - loss 0.62\n",
      "Epoch #4 - loss 0.61\n",
      "Epoch #4 - avg loss 0.60\n",
      "Epoch #5 - loss 0.61\n",
      "Epoch #5 - loss 0.61\n",
      "Epoch #5 - avg loss 0.59\n",
      "Epoch #6 - loss 0.62\n",
      "Epoch #6 - loss 0.57\n",
      "Epoch #6 - avg loss 0.57\n",
      "Epoch #7 - loss 0.47\n",
      "Epoch #7 - loss 0.60\n",
      "Epoch #7 - avg loss 0.60\n",
      "Epoch #8 - loss 0.49\n",
      "Epoch #8 - loss 0.66\n",
      "Epoch #8 - avg loss 0.60\n",
      "Epoch #9 - loss 0.58\n",
      "Epoch #9 - loss 0.56\n",
      "Epoch #9 - avg loss 0.59\n",
      "Epoch #10 - loss 0.65\n",
      "Epoch #10 - loss 0.62\n",
      "Epoch #10 - avg loss 0.58\n",
      "Epoch #11 - loss 0.58\n",
      "Epoch #11 - loss 0.63\n",
      "Epoch #11 - avg loss 0.57\n",
      "Epoch #12 - loss 0.59\n",
      "Epoch #12 - loss 0.61\n",
      "Epoch #12 - avg loss 0.57\n",
      "Epoch #13 - loss 0.57\n",
      "Epoch #13 - loss 0.54\n",
      "Epoch #13 - avg loss 0.57\n",
      "Epoch #14 - loss 0.65\n",
      "Epoch #14 - loss 0.51\n",
      "Epoch #14 - avg loss 0.58\n",
      "Epoch #15 - loss 0.56\n",
      "Epoch #15 - loss 0.57\n",
      "Epoch #15 - avg loss 0.58\n",
      "Epoch #16 - loss 0.52\n",
      "Epoch #16 - loss 0.51\n",
      "Epoch #16 - avg loss 0.57\n",
      "Epoch #17 - loss 0.78\n",
      "Epoch #17 - loss 0.60\n",
      "Epoch #17 - avg loss 0.56\n",
      "Epoch #18 - loss 0.82\n",
      "Epoch #18 - loss 0.53\n",
      "Epoch #18 - avg loss 0.55\n",
      "Epoch #19 - loss 0.40\n",
      "Epoch #19 - loss 0.66\n",
      "Epoch #19 - avg loss 0.54\n",
      "Epoch #20 - loss 0.53\n",
      "Epoch #20 - loss 0.51\n",
      "Epoch #20 - avg loss 0.55\n",
      "Epoch #21 - loss 0.56\n",
      "Epoch #21 - loss 0.54\n",
      "Epoch #21 - avg loss 0.57\n",
      "Epoch #22 - loss 0.55\n",
      "Epoch #22 - loss 0.80\n",
      "Epoch #22 - avg loss 0.56\n",
      "Epoch #23 - loss 0.43\n",
      "Epoch #23 - loss 0.47\n",
      "Epoch #23 - avg loss 0.53\n",
      "Epoch #24 - loss 0.48\n",
      "Epoch #24 - loss 0.60\n",
      "Epoch #24 - avg loss 0.53\n",
      "Epoch #25 - loss 0.52\n",
      "Epoch #25 - loss 0.58\n",
      "Epoch #25 - avg loss 0.55\n",
      "Epoch #26 - loss 0.55\n",
      "Epoch #26 - loss 0.55\n",
      "Epoch #26 - avg loss 0.59\n",
      "Epoch #27 - loss 0.74\n",
      "Epoch #27 - loss 0.52\n",
      "Epoch #27 - avg loss 0.54\n",
      "Epoch #28 - loss 0.56\n",
      "Epoch #28 - loss 0.46\n",
      "Epoch #28 - avg loss 0.52\n",
      "Epoch #29 - loss 0.55\n",
      "Epoch #29 - loss 0.58\n",
      "Epoch #29 - avg loss 0.54\n",
      "Epoch #30 - loss 0.56\n",
      "Epoch #30 - loss 0.66\n",
      "Epoch #30 - avg loss 0.53\n",
      "Epoch #31 - loss 0.48\n",
      "Epoch #31 - loss 0.62\n",
      "Epoch #31 - avg loss 0.53\n",
      "Epoch #32 - loss 0.53\n",
      "Epoch #32 - loss 0.59\n",
      "Epoch #32 - avg loss 0.55\n",
      "Epoch #33 - loss 0.49\n",
      "Epoch #33 - loss 0.53\n",
      "Epoch #33 - avg loss 0.52\n",
      "Epoch #34 - loss 0.52\n",
      "Epoch #34 - loss 0.49\n",
      "Epoch #34 - avg loss 0.51\n",
      "Epoch #35 - loss 0.48\n",
      "Epoch #35 - loss 0.52\n",
      "Epoch #35 - avg loss 0.50\n",
      "Epoch #36 - loss 0.56\n",
      "Epoch #36 - loss 0.55\n",
      "Epoch #36 - avg loss 0.51\n",
      "Epoch #37 - loss 0.50\n",
      "Epoch #37 - loss 0.62\n",
      "Epoch #37 - avg loss 0.52\n",
      "Epoch #38 - loss 0.40\n",
      "Epoch #38 - loss 0.69\n",
      "Epoch #38 - avg loss 0.53\n",
      "Epoch #39 - loss 0.51\n",
      "Epoch #39 - loss 0.64\n",
      "Epoch #39 - avg loss 0.54\n",
      "Epoch #40 - loss 0.61\n",
      "Epoch #40 - loss 0.51\n",
      "Epoch #40 - avg loss 0.52\n",
      "Epoch #41 - loss 0.47\n",
      "Epoch #41 - loss 0.46\n",
      "Epoch #41 - avg loss 0.51\n",
      "Epoch #42 - loss 0.51\n",
      "Epoch #42 - loss 0.48\n",
      "Epoch #42 - avg loss 0.52\n",
      "Epoch #43 - loss 0.63\n",
      "Epoch #43 - loss 0.54\n",
      "Epoch #43 - avg loss 0.51\n",
      "Epoch #44 - loss 0.48\n",
      "Epoch #44 - loss 0.49\n",
      "Epoch #44 - avg loss 0.50\n",
      "Epoch #45 - loss 0.41\n",
      "Epoch #45 - loss 0.48\n",
      "Epoch #45 - avg loss 0.48\n",
      "Epoch #46 - loss 0.52\n",
      "Epoch #46 - loss 0.52\n",
      "Epoch #46 - avg loss 0.49\n",
      "Epoch #47 - loss 0.48\n",
      "Epoch #47 - loss 0.45\n",
      "Epoch #47 - avg loss 0.51\n",
      "Epoch #48 - loss 0.52\n",
      "Epoch #48 - loss 0.55\n",
      "Epoch #48 - avg loss 0.48\n",
      "Epoch #49 - loss 0.43\n",
      "Epoch #49 - loss 0.43\n",
      "Epoch #49 - avg loss 0.49\n",
      "Epoch #50 - loss 0.41\n",
      "Epoch #50 - loss 0.65\n",
      "Epoch #50 - avg loss 0.50\n",
      "Epoch #51 - loss 0.47\n",
      "Epoch #51 - loss 0.55\n",
      "Epoch #51 - avg loss 0.51\n",
      "Epoch #52 - loss 0.69\n",
      "Epoch #52 - loss 0.48\n",
      "Epoch #52 - avg loss 0.50\n",
      "Epoch #53 - loss 0.57\n",
      "Epoch #53 - loss 0.42\n",
      "Epoch #53 - avg loss 0.50\n",
      "Epoch #54 - loss 0.44\n",
      "Epoch #54 - loss 0.45\n",
      "Epoch #54 - avg loss 0.49\n",
      "Epoch #55 - loss 0.46\n",
      "Epoch #55 - loss 0.48\n",
      "Epoch #55 - avg loss 0.47\n",
      "Epoch #56 - loss 0.43\n",
      "Epoch #56 - loss 0.54\n",
      "Epoch #56 - avg loss 0.46\n",
      "Epoch #57 - loss 0.46\n",
      "Epoch #57 - loss 0.47\n",
      "Epoch #57 - avg loss 0.47\n",
      "Epoch #58 - loss 0.54\n",
      "Epoch #58 - loss 0.47\n",
      "Epoch #58 - avg loss 0.47\n",
      "Epoch #59 - loss 0.42\n",
      "Epoch #59 - loss 0.50\n",
      "Epoch #59 - avg loss 0.51\n",
      "Epoch #60 - loss 0.42\n",
      "Epoch #60 - loss 0.51\n",
      "Epoch #60 - avg loss 0.50\n",
      "Epoch #61 - loss 0.45\n",
      "Epoch #61 - loss 0.45\n",
      "Epoch #61 - avg loss 0.49\n",
      "Epoch #62 - loss 0.43\n",
      "Epoch #62 - loss 0.48\n",
      "Epoch #62 - avg loss 0.47\n",
      "Epoch #63 - loss 0.49\n",
      "Epoch #63 - loss 0.58\n",
      "Epoch #63 - avg loss 0.47\n",
      "Epoch #64 - loss 0.48\n",
      "Epoch #64 - loss 0.45\n",
      "Epoch #64 - avg loss 0.46\n",
      "Epoch #65 - loss 0.43\n",
      "Epoch #65 - loss 0.55\n",
      "Epoch #65 - avg loss 0.47\n",
      "Epoch #66 - loss 0.59\n",
      "Epoch #66 - loss 0.48\n",
      "Epoch #66 - avg loss 0.49\n",
      "Epoch #67 - loss 0.48\n",
      "Epoch #67 - loss 0.58\n",
      "Epoch #67 - avg loss 0.46\n",
      "Epoch #68 - loss 0.44\n",
      "Epoch #68 - loss 0.49\n",
      "Epoch #68 - avg loss 0.46\n",
      "Epoch #69 - loss 0.49\n",
      "Epoch #69 - loss 0.64\n",
      "Epoch #69 - avg loss 0.47\n",
      "Epoch #70 - loss 0.42\n",
      "Epoch #70 - loss 0.55\n",
      "Epoch #70 - avg loss 0.47\n",
      "Epoch #71 - loss 0.46\n",
      "Epoch #71 - loss 0.55\n",
      "Epoch #71 - avg loss 0.47\n",
      "Epoch #72 - loss 0.45\n",
      "Epoch #72 - loss 0.40\n",
      "Epoch #72 - avg loss 0.43\n",
      "Epoch #73 - loss 0.49\n",
      "Epoch #73 - loss 0.50\n",
      "Epoch #73 - avg loss 0.45\n",
      "Epoch #74 - loss 0.37\n",
      "Epoch #74 - loss 0.54\n",
      "Epoch #74 - avg loss 0.46\n",
      "Epoch #75 - loss 0.49\n",
      "Epoch #75 - loss 0.45\n",
      "Epoch #75 - avg loss 0.47\n",
      "Epoch #76 - loss 0.38\n",
      "Epoch #76 - loss 0.55\n",
      "Epoch #76 - avg loss 0.45\n",
      "Epoch #77 - loss 0.45\n",
      "Epoch #77 - loss 0.41\n",
      "Epoch #77 - avg loss 0.45\n",
      "Epoch #78 - loss 0.43\n",
      "Epoch #78 - loss 0.43\n",
      "Epoch #78 - avg loss 0.45\n",
      "Epoch #79 - loss 0.41\n",
      "Epoch #79 - loss 0.67\n",
      "Epoch #79 - avg loss 0.45\n",
      "Epoch #80 - loss 0.43\n",
      "Epoch #80 - loss 0.44\n",
      "Epoch #80 - avg loss 0.44\n",
      "Epoch #81 - loss 0.40\n",
      "Epoch #81 - loss 0.48\n",
      "Epoch #81 - avg loss 0.46\n",
      "Epoch #82 - loss 0.43\n",
      "Epoch #82 - loss 0.35\n",
      "Epoch #82 - avg loss 0.45\n",
      "Epoch #83 - loss 0.41\n",
      "Epoch #83 - loss 0.43\n",
      "Epoch #83 - avg loss 0.44\n",
      "Epoch #84 - loss 0.37\n",
      "Epoch #84 - loss 0.41\n",
      "Epoch #84 - avg loss 0.45\n",
      "Epoch #85 - loss 0.40\n",
      "Epoch #85 - loss 0.44\n",
      "Epoch #85 - avg loss 0.45\n",
      "Epoch #86 - loss 0.45\n",
      "Epoch #86 - loss 0.45\n",
      "Epoch #86 - avg loss 0.44\n",
      "Epoch #87 - loss 0.39\n",
      "Epoch #87 - loss 0.48\n",
      "Epoch #87 - avg loss 0.44\n",
      "Epoch #88 - loss 0.47\n",
      "Epoch #88 - loss 0.41\n",
      "Epoch #88 - avg loss 0.42\n",
      "Epoch #89 - loss 0.35\n",
      "Epoch #89 - loss 0.49\n",
      "Epoch #89 - avg loss 0.43\n",
      "Epoch #90 - loss 0.41\n",
      "Epoch #90 - loss 0.47\n",
      "Epoch #90 - avg loss 0.44\n",
      "Epoch #91 - loss 0.38\n",
      "Epoch #91 - loss 0.42\n",
      "Epoch #91 - avg loss 0.43\n",
      "Epoch #92 - loss 0.39\n",
      "Epoch #92 - loss 0.43\n",
      "Epoch #92 - avg loss 0.44\n",
      "Epoch #93 - loss 0.45\n",
      "Epoch #93 - loss 0.47\n",
      "Epoch #93 - avg loss 0.43\n",
      "Epoch #94 - loss 0.38\n",
      "Epoch #94 - loss 0.47\n",
      "Epoch #94 - avg loss 0.43\n",
      "Epoch #95 - loss 0.42\n",
      "Epoch #95 - loss 0.37\n",
      "Epoch #95 - avg loss 0.45\n",
      "Epoch #96 - loss 0.42\n",
      "Epoch #96 - loss 0.41\n",
      "Epoch #96 - avg loss 0.45\n",
      "Epoch #97 - loss 0.41\n",
      "Epoch #97 - loss 0.39\n",
      "Epoch #97 - avg loss 0.45\n",
      "Epoch #98 - loss 0.43\n",
      "Epoch #98 - loss 0.40\n",
      "Epoch #98 - avg loss 0.42\n",
      "Epoch #99 - loss 0.37\n",
      "Epoch #99 - loss 0.36\n",
      "Epoch #99 - avg loss 0.41\n"
     ]
    }
   ],
   "source": [
    "train_dataset =  QVDataset(\"qvhighlights_features\\\\text_features\", \"qvhighlights_features\\\\video_features\", \"qvhighlights_features\\\\highlight_train_release.jsonl\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=pad_collate)\n",
    "config = DetrConfig(\n",
    "    d_model=256,\n",
    "    encoder_layers=2,\n",
    "    decoder_layers=2,\n",
    "    num_queries=10,\n",
    "    # TODO: figure out dropout details\n",
    "    dropout=0.1,\n",
    "    activation_dropout=0.1,\n",
    "    bbox_cost=10,\n",
    "    giou_cost=1,\n",
    "    class_cost=4,\n",
    "    giou_loss_coefficient=1,\n",
    "    bbox_loss_coefficient=10,\n",
    "    num_labels=1\n",
    ")\n",
    "\n",
    "momentDETR_model = MomentDetr(config).to(\"cuda\")\n",
    "epochs = 100\n",
    "optimizer = torch.optim.AdamW(momentDETR_model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 400)\n",
    "print_every = 100\n",
    "for epoch in range(epochs):\n",
    "    momentDETR_model.train()\n",
    "    batch_losses = []\n",
    "    for idx, data in enumerate(train_loader):\n",
    "        data = put_batch_on_device(data, \"cuda\")\n",
    "        optimizer.zero_grad()\n",
    "        _, _, loss = momentDETR_model(**data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_losses.append(loss.detach().cpu().item())\n",
    "        if idx % print_every == 0:\n",
    "            print(f\"Epoch #{epoch} - loss {batch_losses[-1]:.2f}\")\n",
    "    print(f\"Epoch #{epoch} - avg loss {np.mean(batch_losses):.2f}\")\n",
    "    scheduler.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentDETR_model.eval()\n",
    "\n",
    "val_dataset = QVDataset(\"qvhighlights_features/text_features\",\n",
    "                        \"qvhighlights_features/video_features\",\n",
    "                        \"qvhighlights_features/highlight_val_release.jsonl\")\n",
    "val_loader = DataLoader(train_dataset, collate_fn=pad_collate, shuffle=True)\n",
    "batch = put_batch_on_device(next(iter(val_loader)), \"cuda\")\n",
    "a, b, c = momentDETR_model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentDETR_model.save_pretrained(\"moment-detr-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[64., 78.]], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"][0][\"boxes\"] * 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[63.8942, 81.5064]], device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = b.softmax(-1)\n",
    "indices = torch.where(probs[:, :, 0] > 0.5)\n",
    "a[indices] * 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_features': tensor([[[ 0.3393,  0.1165,  0.1020,  ...,  0.2468,  0.5906,  0.1013],\n",
       "          [ 1.9753, -0.5844,  0.3685,  ...,  1.1658,  0.8050, -0.9801],\n",
       "          [ 1.1319, -0.2811, -0.4251,  ...,  1.1118, -0.7140, -1.5363],\n",
       "          ...,\n",
       "          [ 1.6926, -1.6952, -1.8157,  ...,  0.5784, -0.1449,  1.3623],\n",
       "          [ 1.3815, -2.8677, -2.3773,  ...,  0.3917,  0.0104,  1.5142],\n",
       "          [ 1.5898, -1.9487, -1.7853,  ...,  0.5196,  0.5867,  1.0427]]],\n",
       "        device='cuda:0'),\n",
       " 'text_attn_mask': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1.]], device='cuda:0'),\n",
       " 'video_features': tensor([[[ 0.6747,  0.5734,  0.3003,  ..., -0.6585,  0.0275,  0.6033],\n",
       "          [ 0.6113,  0.5741,  0.1799,  ..., -0.4763, -0.0072,  0.5828],\n",
       "          [ 0.6008,  0.4625,  0.3425,  ...,  0.2027,  0.2525,  0.9696],\n",
       "          ...,\n",
       "          [ 1.0498,  0.3782,  0.3071,  ..., -0.5147,  0.5142,  1.1265],\n",
       "          [ 0.9453,  0.3807, -0.1985,  ..., -0.4939,  0.1384,  1.0129],\n",
       "          [ 0.8916,  0.7292,  0.0933,  ..., -0.6016,  0.1888,  1.2447]]],\n",
       "        device='cuda:0'),\n",
       " 'video_attn_mask': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1.]], device='cuda:0'),\n",
       " 'labels': [{'boxes': tensor([[0.4267, 0.5200]], device='cuda:0'),\n",
       "   'class_labels': tensor([0], device='cuda:0')}]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.9702e-06, 1.0000e+00],\n",
       "         [3.2925e-06, 1.0000e+00],\n",
       "         [1.0000e+00, 1.7850e-06],\n",
       "         [4.4461e-05, 9.9996e-01],\n",
       "         [3.1897e-06, 1.0000e+00],\n",
       "         [1.3979e-04, 9.9986e-01],\n",
       "         [6.6539e-06, 9.9999e-01],\n",
       "         [5.5409e-06, 9.9999e-01],\n",
       "         [4.6470e-06, 1.0000e+00],\n",
       "         [5.1168e-06, 9.9999e-01]]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x0000021B76786CE0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "momentDETR_model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6386949"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(param.numel() for param in momentDETR_model.parameters())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
