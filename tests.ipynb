{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP\n",
    "\n",
    "Inference using OpeniAI's [CLIP](https://github.com/openai/CLIP) repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ioanr\\miniconda3\\envs\\master\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ioanr\\miniconda3\\envs\\master\\Lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    }
   ],
   "source": [
    "text = clip.tokenize([\"a dog is sitting on a bench in the park\"]).to(device)\n",
    "text_features = model.encode_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETR\n",
    "\n",
    "DETR Inference through the HuggingFace `transformers` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "from PIL import Image, ImageDraw\n",
    "import requests\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ioanr\\miniconda3\\envs\\master\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "image_processor = DetrImageProcessor()\n",
    "detr_model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[-1.5014, -1.5185, -1.5699,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.5014, -1.5185, -1.5699,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.5528, -1.5528, -1.5699,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [-1.5185, -1.5185, -1.5014,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.4500, -1.4672, -1.4672,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.3987, -1.4158, -1.4329,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-1.4055, -1.4230, -1.4755,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.4055, -1.4230, -1.4755,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.4580, -1.4580, -1.4755,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [-1.4230, -1.4230, -1.4055,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.3529, -1.3704, -1.3704,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.3004, -1.3179, -1.3354,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-1.1770, -1.1944, -1.2467,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.1770, -1.1944, -1.2467,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.2293, -1.2293, -1.2467,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [-1.1944, -1.1944, -1.1770,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.1247, -1.1421, -1.1421,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.0724, -1.0898, -1.1073,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-2.1179, -2.1008, -2.0665,  ..., -1.6042, -1.6213, -1.6384],\n",
       "          [-2.1179, -2.1008, -2.0837,  ..., -1.5870, -1.6042, -1.6213],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -1.5699, -1.5870, -1.5870],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-2.0007, -1.9832, -1.9482,  ..., -1.4755, -1.4930, -1.5105],\n",
       "          [-2.0007, -2.0007, -1.9657,  ..., -1.4755, -1.4930, -1.5105],\n",
       "          [-2.0182, -2.0182, -2.0007,  ..., -1.4755, -1.4930, -1.4930],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-1.8044, -1.7870, -1.7696,  ..., -1.4384, -1.4559, -1.4733],\n",
       "          [-1.8044, -1.7870, -1.7870,  ..., -1.4384, -1.4559, -1.4733],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.4559, -1.4733, -1.4733],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]]), 'pixel_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = [\n",
    "    Image.open(requests.get(\n",
    "        \"https://farm5.staticflickr.com/4029/4669549715_7db3735de0_z.jpg\", stream=True).raw),\n",
    "    Image.open(requests.get(\n",
    "        \"https://farm3.staticflickr.com/2050/2407157255_5ac59d6ebc_z.jpg\", stream=True).raw)\n",
    "]\n",
    "\n",
    "processed_image = image_processor.preprocess(\n",
    "    images, return_tensors=\"pt\")\n",
    "processed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "detr_model.eval()\n",
    "with torch.no_grad():\n",
    "    output = detr_model(**processed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'scores': tensor([0.8315, 0.9965, 0.9988, 0.9770, 0.9976]),\n",
       "  'labels': tensor([32,  2, 18, 15,  1]),\n",
       "  'boxes': tensor([[ 4.4946e-01,  4.1292e-01,  4.6387e-01,  4.4605e-01],\n",
       "          [ 3.8136e-01,  4.5823e-01,  4.9195e-01,  7.0207e-01],\n",
       "          [ 8.9242e-01,  6.1414e-01,  9.8079e-01,  6.6543e-01],\n",
       "          [-1.6114e-04,  7.6959e-01,  6.3832e-01,  9.4864e-01],\n",
       "          [ 3.8834e-01,  3.6991e-01,  4.9615e-01,  6.6282e-01]])},\n",
       " {'scores': tensor([0.5345, 0.8457, 0.5441, 0.9088, 0.6941, 0.7361, 0.9940, 0.9704, 0.8768,\n",
       "          0.9809, 0.9995, 0.8063, 0.6922, 0.8574, 0.9768, 0.5396, 0.9695]),\n",
       "  'labels': tensor([27, 10, 10, 77,  1,  1, 77,  1, 10,  1,  1, 27,  1,  1,  1,  1,  1]),\n",
       "  'boxes': tensor([[-2.1952e-04,  8.1461e-01,  1.6846e-01,  9.9997e-01],\n",
       "          [ 2.6563e-01,  1.0263e-01,  3.0258e-01,  2.1878e-01],\n",
       "          [ 1.0063e-01,  3.3650e-04,  2.9782e-01,  7.4236e-02],\n",
       "          [ 6.2298e-01,  7.2955e-01,  6.6983e-01,  8.2029e-01],\n",
       "          [ 8.2300e-01,  3.2762e-01,  9.9988e-01,  9.9307e-01],\n",
       "          [ 6.4149e-05,  5.0974e-01,  1.5826e-01,  8.4766e-01],\n",
       "          [ 2.5549e-01,  3.9200e-01,  2.8314e-01,  4.9827e-01],\n",
       "          [ 9.6366e-02,  5.6552e-01,  1.8740e-01,  8.5620e-01],\n",
       "          [ 2.2257e-01,  8.9303e-02,  2.6393e-01,  2.2189e-01],\n",
       "          [-1.7836e-04,  5.1102e-01,  1.7374e-01,  9.9504e-01],\n",
       "          [ 1.6923e-01,  2.0496e-01,  5.8954e-01,  9.9127e-01],\n",
       "          [-2.6346e-04,  8.2621e-01,  1.1953e-01,  9.9973e-01],\n",
       "          [ 8.3344e-01,  4.1757e-01,  1.0001e+00,  9.9587e-01],\n",
       "          [ 9.9782e-02,  5.6599e-01,  1.8049e-01,  7.0925e-01],\n",
       "          [ 5.4717e-01,  5.0170e-01,  6.6618e-01,  7.4326e-01],\n",
       "          [ 6.2045e-01,  2.6937e-01,  9.8682e-01,  9.9256e-01],\n",
       "          [ 5.0160e-01,  2.5543e-01,  9.9649e-01,  9.9336e-01]])}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_processed = image_processor.post_process_object_detection(output)\n",
    "post_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class is bicycle\n"
     ]
    }
   ],
   "source": [
    "pred_label = post_processed[0][\"labels\"][1].item()\n",
    "print(\"Predicted class is\", detr_model.config.id2label[pred_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_draw = ImageDraw.Draw(images[0])\n",
    "rect = post_processed[0][\"boxes\"].tolist()[1]\n",
    "rect = [e * (images[0].width if idx % 2 == 0 else images[0].height) for idx, e in enumerate(rect)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_draw.rectangle(rect, outline=\"blue\", width=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface CLIP\n",
    "\n",
    "Inference using the CLIP model provided by the Huggingface `transformers` library (should yiled simillar results as directlly using OpenAI's repo but output is richer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ioanr\\miniconda3\\envs\\master\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "texts = [\"image of a bird\", \"image of a cat\", \"image of a dog\", \"image containing one or more cats\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs = processor.image_processor(image, return_tensors=\"pt\")\n",
    "image_features = model.get_image_features(**inputs)\n",
    "image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(text=texts, images=image, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[18.0458],\n",
       "        [24.8410],\n",
       "        [20.0701],\n",
       "        [28.1160]], grad_fn=<MulBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response.logits_per_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 512])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response.text_model_output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = model.get_text_features(**{k:v for k, v in inputs.items() if k != \"pixel_values\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
