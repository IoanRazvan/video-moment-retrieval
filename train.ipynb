{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from video_moment_retrieval.utils.logging import init_logging, logger\n",
    "from video_moment_retrieval.datasets.qv_highlights import QVDataset, pad_collate\n",
    "from video_moment_retrieval.moment_detr.model import VideoDetrConfig, MomentDetr\n",
    "from transformers import TrainingArguments, Trainer, EvalPrediction\n",
    "\n",
    "init_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import numpy.typing as npt\n",
    "from scipy.special import softmax\n",
    "from video_moment_retrieval.detr_matcher.matcher import center_to_edges\n",
    "from video_moment_retrieval.eval.eval import compute_mr_ap\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def process_preds_and_labels(eval_preds: EvalPrediction) -> tuple[list[dict[str, Any]], list[list[float]]]:\n",
    "    # eval.label_ids -> list[list[dict[str, np.array]]]\n",
    "    # eval.predictions -> list[tuple[np.array, np.array]]\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    for batch_idx in range(len(eval_preds.label_ids)):\n",
    "        batch_labels = eval_preds.label_ids[batch_idx]\n",
    "        batch_predictions = eval_preds.predictions[batch_idx]\n",
    "        # moments and scores are each batch_size x 10 x 2\n",
    "        moments, scores = batch_predictions\n",
    "        scores = softmax(scores, -1)[..., 0]  # batch_size x 10\n",
    "        \n",
    "        for video_labels, video_predictions, video_scores in zip(batch_labels, moments, scores):\n",
    "            qid_label, qid_prediction = [], []\n",
    "            gt_windows = center_to_edges(video_labels[\"boxes\"]) * video_labels[\"duration\"]\n",
    "            pred_windows = center_to_edges(video_predictions) * video_labels[\"duration\"]\n",
    "            pred_windows = np.round(pred_windows / 2, 0) * 2\n",
    "            pred_windows = np.clip(pred_windows, a_min=0, a_max=150)\n",
    "            \n",
    "            qid_prediction = [(window[0].item(), window[1].item(), score) for window, score in zip(pred_windows, video_scores)]\n",
    "            qid_label = [(window[0].item(), window[1].item()) for window in gt_windows]\n",
    "            \n",
    "        \n",
    "            labels.append(qid_label)\n",
    "            predictions.append(qid_prediction)\n",
    "    \n",
    "    return labels, predictions  \n",
    "\n",
    "def compute_metrics(eval_preds: EvalPrediction):\n",
    "    labels, predictions = process_preds_and_labels(eval_preds)\n",
    "    metrics_dict = compute_mr_ap(predictions, labels, num_workers=8)\n",
    "    \n",
    "    return {\n",
    "        \"mAP@0.5\": metrics_dict[\"0.5\"],\n",
    "        \"mAP@0.7\": metrics_dict[\"0.7\"],\n",
    "        \"mAP\": metrics_dict[\"average\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 09:21:51,109 - INFO video_moment_retrieval - 3165430441.py:23 - Running model using config VideoDetrConfig {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auxiliary_loss\": false,\n",
      "  \"backbone\": \"resnet50\",\n",
      "  \"backbone_config\": null,\n",
      "  \"backbone_kwargs\": {\n",
      "    \"in_chans\": 3,\n",
      "    \"out_indices\": [\n",
      "      1,\n",
      "      2,\n",
      "      3,\n",
      "      4\n",
      "    ]\n",
      "  },\n",
      "  \"bbox_cost\": 10,\n",
      "  \"bbox_loss_coefficient\": 10,\n",
      "  \"ce_loss_coefficient\": 4,\n",
      "  \"class_cost\": 4,\n",
      "  \"d_model\": 256,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 1024,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 2,\n",
      "  \"dice_loss_coefficient\": 1,\n",
      "  \"dilation\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 1024,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 2,\n",
      "  \"eos_coefficient\": 0.1,\n",
      "  \"giou_cost\": 1,\n",
      "  \"giou_loss_coefficient\": 1,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"init_xavier_std\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"mask_loss_coefficient\": 1,\n",
      "  \"model_type\": \"detr\",\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"num_queries\": 10,\n",
      "  \"position_embedding_type\": \"sine\",\n",
      "  \"saliency_loss_coefficient\": 2,\n",
      "  \"text_embedding_dim\": 512,\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_pretrained_backbone\": true,\n",
      "  \"use_timm_backbone\": true,\n",
      "  \"video_embedding_dim\": 512\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099b31ab55444077b7c5ae59825c4390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc4330f08cf47a69a4b7ab8da18ddcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 10.930665016174316, 'eval_mAP@0.5': 0.0, 'eval_mAP@0.7': 0.0, 'eval_mAP': 0.0, 'eval_runtime': 38.4116, 'eval_samples_per_second': 40.352, 'eval_steps_per_second': 5.051, 'epoch': 0.09}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 57\u001b[0m\n\u001b[0;32m     27\u001b[0m train_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./train_output\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     29\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# use_cpu=True\u001b[39;00m\n\u001b[0;32m     46\u001b[0m )\n\u001b[0;32m     48\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     49\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     50\u001b[0m     args\u001b[38;5;241m=\u001b[39mtrain_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[0;32m     55\u001b[0m )\n\u001b[1;32m---> 57\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ioanr\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\video-moment-retrieval-sPitWtGb-py3.10\\lib\\site-packages\\transformers\\trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ioanr\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\video-moment-retrieval-sPitWtGb-py3.10\\lib\\site-packages\\transformers\\trainer.py:2178\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2175\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2177\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 2178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   2179\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[1;32mc:\\Users\\ioanr\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\video-moment-retrieval-sPitWtGb-py3.10\\lib\\site-packages\\accelerate\\data_loader.py:464\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    463\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_blocking)\n\u001b[1;32m--> 464\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[1;32mc:\\Users\\ioanr\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\video-moment-retrieval-sPitWtGb-py3.10\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\ioanr\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\video-moment-retrieval-sPitWtGb-py3.10\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\ioanr\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\video-moment-retrieval-sPitWtGb-py3.10\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\ioanr\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\video-moment-retrieval-sPitWtGb-py3.10\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\master\\disertation\\video-moment-retrieval\\video_moment_retrieval\\datasets\\qv_highlights.py:40\u001b[0m, in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     33\u001b[0m meta \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvid\u001b[39m\u001b[38;5;124m\"\u001b[39m: vid,\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m\"\u001b[39m: qid,\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: item_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m\"\u001b[39m: item_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     38\u001b[0m }\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_features_path, \u001b[38;5;28mstr\u001b[39m(qid) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m np_archive:\n\u001b[1;32m---> 40\u001b[0m     query_features \u001b[38;5;241m=\u001b[39m np_archive[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     41\u001b[0m video_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_features_path, vid \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     43\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-5\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ioanr\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\video-moment-retrieval-sPitWtGb-py3.10\\lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = QVDataset(\"qvhighlights_features\\\\text_features\", \"qvhighlights_features\\\\video_features\", \"qvhighlights_features\\\\highlight_train_release.jsonl\")\n",
    "eval_dataset = QVDataset(\"qvhighlights_features\\\\text_features\", \"qvhighlights_features\\\\video_features\", \"qvhighlights_features\\\\highlight_val_release.jsonl\")\n",
    "\n",
    "config = VideoDetrConfig(\n",
    "    d_model=256,\n",
    "    encoder_layers=2,\n",
    "    encoder_ffn_dim=1024,\n",
    "    decoder_layers=2,\n",
    "    decoder_ffn_dim=1024,\n",
    "    num_queries=10,\n",
    "    dropout=0.1,\n",
    "    activation_dropout=0.1,\n",
    "    giou_cost=1,\n",
    "    bbox_cost=10,\n",
    "    class_cost=4,\n",
    "    giou_loss_coefficient=1,\n",
    "    bbox_loss_coefficient=10,\n",
    "    ce_loss_coefficient=4,\n",
    "    saliency_loss_coefficient=2,\n",
    "    num_labels=1,\n",
    ")\n",
    "logger.info(\"Running model using config %s\", config)\n",
    "\n",
    "model = MomentDetr(config)\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    \"./train_output\",\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"constant_with_warmup\",\n",
    "    warmup_steps=500,\n",
    "    num_train_epochs=200,\n",
    "    save_steps=1000,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=True,\n",
    "    max_grad_norm=0.1,\n",
    "    label_names=[\"labels\"],\n",
    "    weight_decay=1e-4,\n",
    "    eval_do_concat_batches=False,\n",
    "    metric_for_best_model=\"mAP\"\n",
    "    # use_cpu=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    data_collator=pad_collate,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3800, 0.2000]])\n",
      "tensor([[0.5170, 0.1521],\n",
      "        [0.2624, 0.1437],\n",
      "        [0.8812, 0.1701],\n",
      "        [0.6829, 0.2108]], grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from video_moment_retrieval.datasets.qv_highlights import QVDataset, pad_collate\n",
    "\n",
    "val_dataset = QVDataset(\"qvhighlights_features\\\\text_features\", \"qvhighlights_features\\\\video_features\", \"qvhighlights_features\\\\highlight_val_release.jsonl\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, collate_fn=pad_collate, shuffle=True)\n",
    "batch = next(iter(val_loader))\n",
    "model = model.to(\"cpu\")\n",
    "output = model(**batch)\n",
    "print(batch[\"labels\"][0][\"boxes\"])\n",
    "scores = output.logits.softmax(axis=-1)[0, :, 0]\n",
    "moments = output.predicted_moments[0, scores > 0.5, :]\n",
    "print(moments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
